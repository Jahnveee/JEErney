# -*- coding: utf-8 -*-
"""JEErney

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rgGf0vW40tGILF5FGzGO2d4lE3Hw_3BR
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn

"""# **STRESS LEVEL PREDICTOR**"""



from google.colab import files
upload = files.upload()

data = pd.read_csv('StressLevelDataset.csv')

data.head()

"""**Psychological Factors** => 'anxiety_level', 'self_esteem', 'mental_health_history', 'depression',

**Physiological Factors** => 'headache', 'blood_pressure', 'sleep_quality', 'breathing_problem

**Environmental Factors** => 'noise_level', 'living_conditions', 'safety', 'basic_needs',

**Academic Factors** => 'academic_performance', 'study_load', 'teacher_student_relationship', 'future_career_concerns',

**Social Factor**  => 'social_support', 'peer_pressure', 'extracurricular_activities', 'bullying'
"""

data.shape

# Access columns using lists instead of tuples
Physiological_Factors = data[[ 'headache', 'blood_pressure', 'sleep_quality', 'breathing_problem']]
Academic_Factors = data[[ 'academic_performance', 'study_load', 'teacher_student_relationship', 'future_career_concerns']]
Psychological_Factors = data[['anxiety_level', 'self_esteem', 'mental_health_history', 'depression']]
Social_Factors = data[[ 'social_support', 'peer_pressure', 'extracurricular_activities', 'bullying']]
Environmental_Factors = data[['noise_level', 'living_conditions', 'safety', 'basic_needs']]

data.isnull().sum()
#no null columns

print("The dataset consists of ", len(data), "students.")
print("The avarage anxiety level is:", round(data.anxiety_level.mean(),2))
print(len(data.loc[data.mental_health_history == 1]), "students reported a history of mental health issues.")

"""### **The PHQ-9 (Patient Health Questionnaire-9) is a widely used tool for assessing the severity of depression**
With scores ranging from 0 to 27. Scores are typically categorized as follows

Low: Scores from 0 to 9

Medium: Scores from 10 to 19

High: Scores from 20 to 27

"""

depressed = len(data.loc[data["depression"] >= 10])
print(round((depressed/len(data))*100,2), "% of students reported moderate or higher levels of depression.")

"""### **The Rosenberg Self-Esteem Scale measures self-esteem on a range from 0 to 30, with higher scores indicating greater self-esteem.**
Scores are typically categorized as:

low (0-14)

moderate (15-25)

high (26-30).
"""

below_avarage = len(data.loc[data["self_esteem"] < data["self_esteem"].mean()])
print(below_avarage, "students have self esteem below avarage")

"""### **Relationship between stress levels and other factors**"""

relation = data.corr()
plt.figure(figsize=(8, 8))
colormap = sns.color_palette("coolwarm", as_cmap=True)  # Change to your preferred colormap
sns.heatmap(relation.iloc[:-1, -1:], annot=True, cmap=colormap, vmin=-1, vmax=1)
plt.title('Correlation Between Stress level and other factors')
plt.show()



"""###**Analysis based on each factor**"""

#PHYSIOLOGICAL_FACTORS
Physiological_Factors_dict = {}
for column,row in Physiological_Factors.items():
    #People in the upper 40%
    if column == "headache" or column == "breathing problem" or column == "blood_pressure":
        count = (row > np.percentile(Physiological_Factors[column],60))
        #Add the resulting dataset
        Physiological_Factors_dict[column] = count
    #People in the lower 40%
    else:
        count = (row < np.percentile(Physiological_Factors[column],40))
        #Add the resulting dataset
        Physiological_Factors_dict[column] = count

physio_df = pd.DataFrame(Physiological_Factors_dict)
#every row where atleast one value is true|
physio_number = len((physio_df[(physio_df >= 1).sum(axis=1) >= 1]))

#ENVIRONMENTAL_FACTORS
Environmental_Factors_dict = {}
for column,row in Environmental_Factors.items():
    #People in the upper 40%
    if column == "noise_level":
        count = (row > np.percentile(Environmental_Factors[column],60))
        #Add the resulting dataset
        Environmental_Factors_dict[column] = count
    #People in the lower 40%
    else:
        count = (row < np.percentile(Environmental_Factors[column],40))
        #Add the resulting dataset to the dictionary under the column's name
        Environmental_Factors_dict[column] = count
#Create a pandas dataframe
env_df = pd.DataFrame(Environmental_Factors_dict)
#Count every row where atleast one value is true
env_number = len((env_df[(env_df >= 1).sum(axis=1) >= 1]))

#PSYCHOLOGICAL FACTORS
Psychological_Factors_dict = {}
for column,row in Psychological_Factors.items():
    #anxiety or depression-- upper 40%
    if column == "anxiety_level" or column == "depression":
        count = (row > np.percentile(Psychological_Factors[column],60))
        #Add the resulting dataset
        Psychological_Factors_dict[column] = count
    #history with mental health
    elif column == "mental_health_history":
        count = row == 1
        #Add the resulting dataset
        Psychological_Factors_dict[column] = count
    #lower 40% for self-esteem
    else:
        count = (row < np.percentile(Psychological_Factors[column],40))
        #Add the resulting dataset
        Psychological_Factors_dict[column] = count

psych_pos_df = pd.DataFrame(Psychological_Factors_dict)
#every row where atleast one value is true
psych_number = len((psych_pos_df[(psych_pos_df >= 1).sum(axis=1) >= 1]))

#ACADEMIC FACTORS
Academic_Factors_dict = {}
for column,row in Academic_Factors.items():
    #People in the upper 40%
    if column == "study_load" or column == "future_career_concerns":
        count = (row > np.percentile(Academic_Factors[column],60))
        #Add the resulting dataset
        Academic_Factors_dict[column] = count
    #People in the lower 40%
    else:
        count = (row < np.percentile(Academic_Factors[column],40))
        #Add the resulting dataset
        Academic_Factors_dict[column] = count

acad_df = pd.DataFrame(Academic_Factors_dict)
#every row where atleast one value is true
acad_number = len((acad_df[(acad_df >= 1).sum(axis=1) >= 1]))

#SOCIAL FACTORS
Social_Factors_dict = {}
for column,row in Social_Factors.items():
    #People in the upper 40%
    if column == "peer_pressure" or column == "extracurricular_activities" or column == "bullying":
        count = (row > np.percentile(Social_Factors[column],60))
        #Add the resulting dataset
        Social_Factors_dict[column] = count
    #People in the lower 40%
    else:
        count = (row < np.percentile(Social_Factors[column],40))
        #Add the resulting dataset
        Social_Factors_dict[column] = count

social_df = pd.DataFrame(Social_Factors_dict)
#every row where atleast one value is true
social_number = len((social_df[(social_df >= 1).sum(axis=1) >= 1]))

negativeExp = pd.Series([psych_number, physio_number, env_number, acad_number, social_number])
col_names = ["Psychological", "Physiological", "Environmental", "Academic", "Social"]
plt.figure(figsize=(10, 6))
ax = sns.barplot(x=col_names, y=negativeExp.values, palette="pastel")  # Change palette as needed
ax.set(title="Number of Students with Negative Experiences in Different Fields")
ax.bar_label(ax.containers[0])
ax.set_ylabel("Number of Students")
plt.show()

labels = ['Psychological', 'Physiological', 'Environmental', 'Academic', 'Social']
sizes = [psych_number, physio_number, env_number, acad_number, social_number]
colors = sns.color_palette('pastel')[0:5]
plt.figure(figsize=(7, 7))
plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140)
plt.title('Distribution of Students with Negative Experiences in Different Fields')
plt.axis('equal')  # Equal aspect ratio ensures the pie chart is circular.
plt.show()

"""## **Training ML Datasets**"""

Y = data['stress_level']
X = data[['anxiety_level', 'self_esteem', 'mental_health_history',
       'depression', 'headache', 'blood_pressure', 'sleep_quality',
       'breathing_problem', 'noise_level', 'living_conditions', 'safety',
       'basic_needs', 'academic_performance', 'study_load',
       'teacher_student_relationship', 'future_career_concerns',
       'social_support', 'peer_pressure', 'extracurricular_activities',
       'bullying']]

from sklearn.model_selection import train_test_split
X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.30,random_state=42)

from sklearn.metrics import accuracy_score,roc_curve,roc_auc_score

from sklearn.linear_model import LogisticRegression
print("For the Logistic Regression")
model_lr=LogisticRegression()
model_lr.fit(X_train,Y_train)
y_pred_train = model_lr.predict(X_train)
y_pred_test = model_lr.predict(X_test)
print("The accuracy on the train set is",accuracy_score(Y_train,y_pred_train))
print("The accuracy on the test set is",accuracy_score(Y_test,y_pred_test))

from sklearn.tree import DecisionTreeClassifier
print("For the DecisionTreeClassifier")
model_dt=DecisionTreeClassifier()
model_dt.fit(X_train,Y_train)
y_pred_train = model_dt.predict(X_train)
y_pred_test = model_dt.predict(X_test)
print("The accuracy on the train set is",accuracy_score(Y_train,y_pred_train))
print("The accuracy on the test set is",accuracy_score(Y_test,y_pred_test))

from sklearn.ensemble import RandomForestClassifier
print("For the RandomForestClassifier")
model_rf=RandomForestClassifier()
model_rf.fit(X_train,Y_train)
y_pred_train = model_rf.predict(X_train)
y_pred_test = model_rf.predict(X_test)
print("The accuracy on the train set is",accuracy_score(Y_train,y_pred_train))
print("The accuracy on the test set is",accuracy_score(Y_test,y_pred_test))

from sklearn.neighbors import KNeighborsClassifier
print("For the KNeighborsClassifier")
model_knn=KNeighborsClassifier()
model_knn.fit(X_train,Y_train)
y_pred_train = model_knn.predict(X_train)
y_pred_test = model_knn.predict(X_test)
print("The accuracy on the train set is",accuracy_score(Y_train,y_pred_train))
print("The accuracy on the test set is",accuracy_score(Y_test,y_pred_test))

from sklearn.naive_bayes import GaussianNB
print("For the GaussianNB")
model_gnb=GaussianNB()
model_gnb.fit(X_train,Y_train)
y_pred_train = model_gnb.predict(X_train)
y_pred_test = model_gnb.predict(X_test)
print("The accuracy on the train set is",accuracy_score(Y_train,y_pred_train))
print("The accuracy on the test set is",accuracy_score(Y_test,y_pred_test))

"""# **JEE Mains January Marks-Based Percentile Prediction Tool**"""

from google.colab import files
upload = files.upload()

data_2023 = pd.read_csv('JEErney_2023_jan.csv')

from google.colab import files
upload = files.upload()

data_2024 = pd.read_csv('JEErney_2024_jan.csv')

print("Data 2023:")
print(data_2023.head())
print("\nData 2024:")
print(data_2024.head())

print("\nColumns in Data 2023:", data_2023.columns)
print("Columns in Data 2024:", data_2024.columns)

# Assuming both datasets have the same structure and we want to concatenate them
merged_data = pd.concat([data_2023, data_2024], ignore_index=True)

# Display the first few rows of the merged dataset
print("\nMerged Data:")
print(merged_data.head())

# Save the merged dataset to a new CSV file
merged_file_path = 'Merged_JEE_Marks_vs_Percentile.csv'
merged_data.to_csv(merged_file_path, index=False)

print(f"\nMerged dataset saved to {merged_file_path}")

data=pd.read_csv('Merged_JEE_Marks_vs_Percentile.csv')

from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# Load the merged dataset
merged_file_path = 'Merged_JEE_Marks_vs_Percentile.csv'
merged_data = pd.read_csv(merged_file_path)

# Display the first few rows of the merged dataset
print(merged_data.head())

X = merged_data.drop(columns=['Percentile'])
y = merged_data['Percentile']

X.isnull().sum()

X['Session'] = X['Session'].fillna(X['Session'].mode()[0])

X['Marks'] = X['Marks'].fillna(X['Marks'].mean())

X.isnull().sum()

y.isnull().sum()

y = y.fillna(y.mean())

y.isnull().sum()

from sklearn import preprocessing
le = preprocessing.LabelEncoder()
X['Session'] = le.fit_transform(X['Session'])

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

"""**Decision Tree Regressor**"""

from sklearn.tree import DecisionTreeRegressor
model_dtr=DecisionTreeRegressor(criterion='absolute_error', max_depth=4, min_samples_leaf=5, max_leaf_nodes=10)
model_dtr.fit(X_train,y_train)

model_dtr.score(X_train,y_train)

model_dtr.score(X_test,y_test)

predictions = model_dtr.predict(X_test)

mse = mean_squared_error(y_test, predictions)
r2 = r2_score(y_test, predictions)
print("MSE:", mse)
print("R2:", r2)

"""**Random Forest Regressor**"""

from sklearn.ensemble import RandomForestRegressor
model_rfr=RandomForestRegressor(n_estimators=150, max_depth=4 )
model_rfr.fit(X_train,y_train)

model_rfr.score(X_train,y_train)

model_rfr.score(X_test,y_test)

predictions_rfr = model_rfr.predict(X_test)

mse = mean_squared_error(y_test, predictions_rfr)
r2 = r2_score(y_test, predictions_rfr)
print("MSE:", mse)
print("R2:", r2)

"""**KNN Regressor**"""

from sklearn.neighbors import KNeighborsRegressor
model_knnr=KNeighborsRegressor(n_neighbors=7)
model_knnr.fit(X_train,y_train)

model_knnr.score(X_train,y_train)

model_knnr.score(X_test,y_test)

predictions_knnr = model_knnr.predict(X_test)

mse = mean_squared_error(y_test, predictions_knnr)
r2 = r2_score(y_test, predictions_knnr)
print("MSE:", mse)
print("R2:", r2)

"""**Linear Regressor**"""

from sklearn.linear_model import LinearRegression
model_lrr=LinearRegression()
model_lrr.fit(X_train,y_train)

model_lrr.score(X_train,y_train)

model_lrr.score(X_test,y_test)

predictions_lrr = model_lrr.predict(X_test)

mse = mean_squared_error(y_test, predictions_lrr)
r2 = r2_score(y_test, predictions_lrr)
print("MSE:", mse)
print("R2:", r2)

"""**Gradient Boosting Regressor**"""

from sklearn.ensemble import GradientBoostingRegressor
gbr = GradientBoostingRegressor()
gbr.fit(X_train, y_train)

gbr.score(X_train,y_train)

gbr.score(X_test,y_test)

predictions_gbr = gbr.predict(X_test)

mse = mean_squared_error(y_test, predictions_gbr)
r2 = r2_score(y_test, predictions_gbr)
print("MSE:", mse)
print("R2:", r2)

"""**Linear Regression**

MSE: 3.99

R²: 0.542

*Summary*: The linear regression model shows moderate performance with an MSE of 3.99 and an R² score of 0.542, indicating that it explains about 54.2% of the variance in the data.

**Random Forest**

MSE: 2.79

R²: 0.679

*Summary*: The random forest model performs significantly better, with an MSE of 2.79 and an R² score of 0.679. This suggests that it captures 67.9% of the variance, indicating better predictive power.

**Gradient Boosting**

MSE: 2.533

R²: 0.70964

*Summary*: Gradient boosting demonstrates the best performance among all models, with the lowest MSE of 2.533 and the highest R² score of 0.70964. This model explains approximately 70.96% of the variance in the data, making it the most effective for this dataset.

**Decision Tree**

MSE: 3.369

R²: 0.613

*Summary*: The decision tree model has an MSE of 3.369 and an R² score of 0.613, indicating it explains 61.3% of the variance. While it performs better than linear regression, it is less effective than random forest and gradient boosting.

**K-Nearest Neighbors**

MSE: 3.19

R²: 0.63

*Summary*: The K-nearest neighbors model achieves an MSE of 3.19 and an R² score of 0.63, explaining 63% of the variance in the data. It performs better than linear regression and decision tree but is outperformed by random forest and gradient boosting.

----------------------------------------------------------------------------

***Conclusion***
Among the evaluated models, Gradient Boosting is the best performing model with the lowest MSE and the highest R², making it the most suitable choice for this regression task. Random Forest also performs well and could be considered a strong alternative. Linear Regression and Decision Tree show relatively low

#**JEE Mains April Marks-Based Percentile Prediction Tool**
"""

from google.colab import files
upload = files.upload()

data_2023_april = pd.read_csv('JEErney_2023_apr.csv')

from google.colab import files
upload = files.upload()

data_2024_april = pd.read_csv('JEErney_2024_apr.csv')

data_2024_april.head()

print("\nColumns in Data 2023:", data_2023_april.columns)
print("Columns in Data 2024:", data_2024_april.columns)

# Assuming both datasets have the same structure and we want to concatenate them
merged_data_april = pd.concat([data_2023_april, data_2024_april], ignore_index=True)

# Display the first few rows of the merged dataset
print("\nMerged Data:")
print(merged_data_april.head())

# Save the merged dataset to a new CSV file
merged_file_path = 'Merged_JEE_Marks_vs_Percentile_april.csv'
merged_data.to_csv(merged_file_path, index=False)

print(f"\nMerged dataset saved to {merged_file_path}")

data=pd.read_csv('Merged_JEE_Marks_vs_Percentile_april.csv')

from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# Load the merged dataset
merged_file_path = 'Merged_JEE_Marks_vs_Percentile_april.csv'
merged_data = pd.read_csv(merged_file_path)

# Display the first few rows of the merged dataset
print(merged_data.head())

X = merged_data.drop(columns=['Percentile'])
y = merged_data['Percentile']

X.isnull().sum()

y.isnull().sum()

from sklearn import preprocessing
le = preprocessing.LabelEncoder()
X['Session'] = le.fit_transform(X['Session'])

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

"""**Decision Tree Regressor**"""

from sklearn.tree import DecisionTreeRegressor
model_dtr_april=DecisionTreeRegressor(criterion='absolute_error', max_depth=4, min_samples_leaf=5, max_leaf_nodes=10)
model_dtr_april.fit(X_train,y_train)

model_dtr_april.score(X_train,y_train)

model_dtr_april.score(X_test,y_test)

predictions_dtr_april= model_dtr_april.predict(X_test)

mse = mean_squared_error(y_test, predictions_dtr_april)
r2 = r2_score(y_test, predictions_dtr_april)
print("MSE:", mse)
print("R2:", r2)

"""**Random Forest Regressor**"""

from sklearn.ensemble import RandomForestRegressor
model_rfr_april=RandomForestRegressor(n_estimators=150, max_depth=4 )
model_rfr_april.fit(X_train,y_train)

model_rfr_april.score(X_train,y_train)

model_rfr_april.score(X_test,y_test)

predictions_rfr_april= model_rfr_april.predict(X_test)

mse = mean_squared_error(y_test, predictions_rfr_april)
r2 = r2_score(y_test, predictions_rfr_april)
print("MSE:", mse)
print("R2:", r2)

"""**KNN Regressor**"""

from sklearn.neighbors import KNeighborsRegressor
model_knnr_april=KNeighborsRegressor(n_neighbors=7)
model_knnr_april.fit(X_train,y_train)

model_knnr_april.score(X_train,y_train)

model_knnr_april.score(X_test,y_test)

predictions_knnr_april= model_knnr_april.predict(X_test)

mse = mean_squared_error(y_test, predictions_knnr_april)
r2 = r2_score(y_test, predictions_knnr_april)
print("MSE:", mse)
print("R2:", r2)

"""**Linear Regression**"""

from sklearn.linear_model import LinearRegression
model_lrr_april=LinearRegression()
model_lrr_april.fit(X_train,y_train)

model_lrr_april.score(X_train,y_train)

model_lrr_april.score(X_test,y_test)

predictions_lrr_april= model_knnr_april.predict(X_test)

mse = mean_squared_error(y_test, predictions_lrr_april)
r2 = r2_score(y_test, predictions_lrr_april)
print("MSE:", mse)
print("R2:", r2)

"""**Gradient Boosting Regressor**"""

from sklearn.ensemble import GradientBoostingRegressor
gbr_april = GradientBoostingRegressor()
gbr_april.fit(X_train, y_train)

gbr_april.score(X_train,y_train)

gbr_april.score(X_test,y_test)

predictions_gbr_april= gbr_april.predict(X_test)

mse = mean_squared_error(y_test, predictions_gbr_april)
r2 = r2_score(y_test, predictions_gbr_april)
print("MSE:", mse)
print("R2:", r2)



"""# ***JEE ADVANCED Analysis- Marks(in percentage) Vs Rank***"""

from google.colab import files
upload = files.upload()

import pandas as pd
advanced = pd.read_csv('JEErney_advancedScoreVsRank.csv')

advanced.head()

import matplotlib.pyplot as plt
import seaborn as sns

def compare_years(year1, year2):
    if str(year1) not in advanced.columns or str(year2) not in advanced.columns:
        print(f"One or both of the years {year1}, {year2} are not in the dataset.")
        return

    ranks = advanced['RANK']
    marks_year1 = advanced[str(year1)]
    marks_year2 = advanced[str(year2)]

    plt.figure(figsize=(12, 6))

    plt.plot(ranks, marks_year1, marker='o', label=f'{year1}', linestyle='-', color='b')
    plt.plot(ranks, marks_year2, marker='o', label=f'{year2}', linestyle='--', color='r')

    for rank, mark1, mark2 in zip(ranks, marks_year1, marks_year2):
        plt.text(rank, mark1, f'{mark1}', ha='right', va='bottom')
        plt.text(rank, mark2, f'{mark2}', ha='right', va='top')

    plt.xlabel('Rank')
    plt.ylabel('Marks (%)')
    plt.title(f'Comparison of JEE Advanced Marks (Percentage) for {year1} vs {year2}')
    plt.legend(title='Year')
    plt.gca().invert_xaxis()  # Optional: Invert x-axis to show rank 1 at the top

    plt.show()


#ENTER THE YEARS YOU WANT TO COMPARE
compare_years(2023, 2024)

plt.figure(figsize=(12, 6))
for rank in advanced['RANK']:
    plt.plot(advanced.columns[1:], advanced[advanced['RANK'] == rank].values[0][1:], marker='o', label=f'Rank {rank}')

plt.xlabel('Year')
plt.ylabel('Marks (%)')
plt.title('JEE Advanced Rank vs Marks (Percentage) Over Years')
plt.legend(title='Rank')
plt.xticks(rotation=45)
plt.show()

advanced.head()

# Create a copy of the DataFrame to avoid modifying the original one
advanced_copy = advanced.copy()

# Plot heatmap using the copied DataFrame
plt.figure(figsize=(12, 8))
advanced_copy.set_index('RANK', inplace=True)
sns.heatmap(advanced_copy, cmap='viridis', annot=True, fmt='.1f')

plt.xlabel('Year')
plt.ylabel('Rank')
plt.title('Heatmap of JEE Advanced Marks (Percentage) by Rank and Year')
plt.show()



"""# **JEE ADVANCED ANALYSIS- TYPE OF QUESTION**"""

from google.colab import files
upload = files.upload()

qType = pd.read_csv('JEErney_Adv_typeOfQ.csv')

# Grouped bar chart
plt.figure(figsize=(12, 8))

# Plot a grouped bar chart
qType.set_index('Type of Question').T.plot(kind='bar')
plt.title('Distribution of Question Types Over Years')
plt.xlabel('Year')
plt.ylabel('Number of Questions')
plt.legend(title='Type of Question', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()

#line chart
plt.figure(figsize=(9, 6))

# Plot a line chart
for question_type in qType['Type of Question']:
    plt.plot(qType.columns[1:], qType[qType['Type of Question'] == question_type].values[0][1:], marker='o', label=question_type)

plt.title('Trend of Question Types Over Years')
plt.xlabel('Year')
plt.ylabel('Number of Questions')
plt.legend(title='Type of Question', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()

# heat map
plt.figure(figsize=(10, 6))

# Plot a heatmap
sns.heatmap(qType.set_index('Type of Question').T, annot=True, cmap='coolwarm', cbar_kws={'label': 'Number of Questions'})
plt.title('Heatmap of Question Types Over Years')
plt.xlabel('Year')
plt.ylabel('Type of Question')
plt.tight_layout()
plt.show()

# For a specific year- Pie Chart
plt.figure(figsize=(8, 8))

# Plot a pie chart for 2023
year = 2023
jee_data_year = qType.set_index('Type of Question')[str(year)]
jee_data_year.plot(kind='pie', autopct='%1.1f%%', startangle=140, legend=False)
plt.title('Distribution of Question Types in JEE Advanced for {}'.format(year))
plt.ylabel('')
plt.tight_layout()
plt.show()

"""# **JEE ADVANCED ANALYSIS - Physics Difficulty Level**"""

from google.colab import files
upload = files.upload()

physicsAdv = pd.read_csv('JEErney_Adv_physics.csv')

physicsAdv.head()

# Plot a stacked bar chart
plt.figure(figsize=(10, 6))

physicsAdv.set_index('PHYSICS').plot(kind='bar', stacked=True)
plt.title('Distribution of Physics Question Difficulty Levels Over Years')
plt.xlabel('Year')
plt.ylabel('Number of Questions')
plt.legend(title='Difficulty Level', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()

# Plot a line chart
plt.figure(figsize=(10, 6))

for difficulty in ['Easy', 'Moderate', 'Tough']:
    plt.plot(physicsAdv['PHYSICS'], physicsAdv[difficulty], marker='o', label=difficulty)

plt.title('Trend of Physics Question Difficulty Levels Over Years')
plt.xlabel('Year')
plt.ylabel('Number of Questions')
plt.legend(title='Difficulty Level', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()

# Plot a heatmap
plt.figure(figsize=(10, 6))

sns.heatmap(physicsAdv.set_index('PHYSICS').T, annot=True, cmap='coolwarm', cbar_kws={'label': 'Number of Questions'})
plt.title('Heatmap of Physics Question Difficulty Levels Over Years')
plt.xlabel('Year')
plt.ylabel('Difficulty Level')
plt.tight_layout()
plt.show()

# Plot a pie chart for a specific year
plt.figure(figsize=(5, 5))
year = 2023
physics_year = physicsAdv.set_index('PHYSICS').loc[year]
physics_year.plot(kind='pie', autopct='%1.1f%%', startangle=140, legend=False)
plt.title('Distribution of Physics Question Difficulty Levels in JEE Advanced for {}'.format(year))
plt.ylabel('')
plt.tight_layout()
plt.show()

"""# **JEE ADVANCED ANALYSIS - Chemistry Difficulty Level**"""

from google.colab import files
upload = files.upload()

chemAdv = pd.read_csv('JEErney_Adv_chem.csv')

# Plot a stacked bar chart
plt.figure(figsize=(10, 6))

chemAdv.set_index('CHEMISTRY').plot(kind='bar', stacked=True)
plt.title('Distribution of Chemistry Question Difficulty Levels Over Years')
plt.xlabel('Year')
plt.ylabel('Number of Questions')
plt.legend(title='Difficulty Level', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()

# Plot a line chart
plt.figure(figsize=(10, 6))

for difficulty in ['Easy', 'Moderate', 'Tough']:
    plt.plot(chemAdv['CHEMISTRY'], chemAdv[difficulty], marker='o', label=difficulty)

plt.title('Trend of Chemistry Question Difficulty Levels Over Years')
plt.xlabel('Year')
plt.ylabel('Number of Questions')
plt.legend(title='Difficulty Level', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()

# Plot a heatmap
plt.figure(figsize=(10, 6))

sns.heatmap(chemAdv.set_index('CHEMISTRY').T, annot=True, cmap='coolwarm', cbar_kws={'label': 'Number of Questions'})
plt.title('Heatmap of Chemistry Question Difficulty Levels Over Years')
plt.xlabel('Year')
plt.ylabel('Difficulty Level')
plt.tight_layout()
plt.show()

# Plot a pie chart for a specific year
plt.figure(figsize=(5, 5))
year = 2023
chem_year = chemAdv.set_index('CHEMISTRY').loc[year]
chem_year.plot(kind='pie', autopct='%1.1f%%', startangle=140, legend=False)
plt.title('Distribution of Chemistry Question Difficulty Levels in JEE Advanced for {}'.format(year))
plt.ylabel('')
plt.tight_layout()
plt.show()

"""# **JEE ADVANCED ANALYSIS - Maths Difficulty Level**"""

from google.colab import files
upload = files.upload()

mathsAdv = pd.read_csv('JEErney_Adv_maths.csv')

# Plot a stacked bar chart
plt.figure(figsize=(10, 6))

mathsAdv.set_index('MATHS').plot(kind='bar', stacked=True)
plt.title('Distribution of Maths Question Difficulty Levels Over Years')
plt.xlabel('Year')
plt.ylabel('Number of Questions')
plt.legend(title='Difficulty Level', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()

# Plot a line chart
plt.figure(figsize=(10, 6))

for difficulty in ['Easy', 'Moderate', 'Tough']:
    plt.plot(mathsAdv['MATHS'], mathsAdv[difficulty], marker='o', label=difficulty)

plt.title('Trend of Maths Question Difficulty Levels Over Years')
plt.xlabel('Year')
plt.ylabel('Number of Questions')
plt.legend(title='Difficulty Level', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()

# Plot a heatmap
plt.figure(figsize=(10, 6))

sns.heatmap(mathsAdv.set_index('MATHS').T, annot=True, cmap='coolwarm', cbar_kws={'label': 'Number of Questions'})
plt.title('Heatmap of Maths Question Difficulty Levels Over Years')
plt.xlabel('Year')
plt.ylabel('Difficulty Level')
plt.tight_layout()
plt.show()

# Plot a pie chart for a specific year
plt.figure(figsize=(5, 5))
year = 2023
maths_year = mathsAdv.set_index('MATHS').loc[year]
maths_year.plot(kind='pie', autopct='%1.1f%%', startangle=140, legend=False)
plt.title('Distribution of Maths Question Difficulty Levels in JEE Advanced for {}'.format(year))
plt.ylabel('')
plt.tight_layout()
plt.show()

